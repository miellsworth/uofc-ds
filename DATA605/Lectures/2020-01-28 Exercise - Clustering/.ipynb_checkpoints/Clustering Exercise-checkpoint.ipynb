{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Original Notebook from Eytan Adar's SI 370 course — University of Michigan)*\n",
    "\n",
    "## Objectives:\n",
    "- Be able to perform hierarchical clustering and k-means clustering.\n",
    "- Know how and when to use different similarity functions, including Euclidean, Manhattan, Jaccard, and Cosine.\n",
    "- Be able to appropriately evaluate clustering results, with or without ground truth.\n",
    "- Be able to determine the number of clusters given a dataset without ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter all warnings.\n",
    "# spurious warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.spatial.distance as spd\n",
    "import scipy.cluster.hierarchy as sph\n",
    "import sklearn as sk\n",
    "import sklearn.cluster as skc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='white', color_codes=True, font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a toy dataset that has two dimensions. There are three obvious clusters.\n",
    "\n",
    "Reference: https://github.com/herrfz/dataanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.randn(12) * 0.2 + np.repeat(np.arange(3)+1, 4)\n",
    "y = np.random.randn(12) * 0.2 + np.repeat(np.array([1,2,1]), 4)\n",
    "plt.scatter(x, y, s=70)\n",
    "for i, xi in enumerate(x):\n",
    "    plt.annotate(str(i), (xi+0.03, y[i]+0.03), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a DataFrame.\n",
    "df_xy = pd.DataFrame({'x': x, 'y': y})\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To perform hierarchical clustering, the first step is to compute \n",
    "# the distance matrix. We will be using the Euclidean distance.\n",
    "dist_xy = spd.squareform(spd.pdist(df_xy, metric='euclidean'))\n",
    "dist_xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The matrix may be too large to display. So let's display a top-left\n",
    "# portion of it.\n",
    "dist_xy[:4,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the matrix above, the diagnonal values are 0, which is\n",
    "expected: an object's distance to itself must be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing hierarchical clustering, let us review an important concept, \"linkage function.\"\n",
    " - For hierarchical clustering, a _linkage function_ is a distance function between two clusters. In scipy multiple different linkage functions are implemented, including single, complete, average, weighted, centroid, median, and ward. We will be using \"single\", which is taking the distance between two nearest points as the distance between two clusters. This is the default option. See [scipy documentation](http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.cluster.hierarchy.linkage.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering and plot the dendrogram.\n",
    "Z_xy = sph.linkage(dist_xy, method='single')  # obtain the linkage matrix\n",
    "_ = sph.dendrogram(Z_xy)  # plot the linkage matrix as a dendrogram\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.suptitle('Dendrogram: 12-point toy data', \n",
    "             fontweight='bold', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dendrogram, the y-axis depicts the distance at which a cluster is formed. This distance is obtained by evaluating the linkage function. We can see that the green cluster forms at around 1.0, then the next cluster forms at around 2.4. There is a large gap (on the y-axis) in between, which indicates that segmenting the data there may be a good choice. In fact, the scipy package automatically selects a threshold, below which the clusters are colored differently, and above which the clusters are all colored blue. This happens to match our interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above analysis, 2.0 seems to be a good distance threshold to \"clip\" the dendrogram and obtain our clustering results. The way to apply the clipping threshold is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_labels = sph.fcluster(Z_xy, 2.0, criterion='distance')\n",
    "df_xy['cluster_label'] = cluster_labels\n",
    "df_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the cluster_labels with the scatterplot at the beginning, we can see that hierarchical clustering indeed captures the actual clusters as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn. Do the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "- Step 1: Import the Iris dataset and create a scatterplot, showing petal_lengths and petal_widths.\n",
    "- Step 2: Compute the distance matrix among irises using petal_lengths and petal_widths. Use Euclidean distance.\n",
    "- Step 3: Perform hierarchical clustering.\n",
    "- Step 4: Pick a distance threshold that results in 3 clusters that match the actual number of species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Import the iris dataset and create scatterplot.\n",
    "# Show only petal_length and petal_width.\n",
    "df_iris = sns.load_dataset('iris')\n",
    "sns.lmplot(x='petal_length', y='petal_width', hue='species', \n",
    "           fit_reg=False, data=df_iris);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: Compute distance matrix. Using petal_length and petal_width only.\n",
    "# plpw means \"petal_length and petal_width\"\n",
    "dist_iris_plpw = spd.squareform(spd.pdist(\n",
    "        df_iris[['petal_length', 'petal_width']], \n",
    "        metric='euclidean'))\n",
    "dist_iris_plpw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering and plot the dendrogram.\n",
    "iris_xy = sph.linkage(dist_iris_plpw, method='single')  # obtain the linkage matrix\n",
    "_ = sph.dendrogram(iris_xy)  # plot the linkage matrix as a dendrogram\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.suptitle('Dendrogram: 12-point toy data', \n",
    "             fontweight='bold', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add  labels back to the dataframe\n",
    "iris_labels = sph.fcluster(iris_xy, 2.0, criterion='distance')\n",
    "df_iris['cluster_label'] = iris_labels\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "sns.lmplot(x='petal_length', y='petal_width', hue='cluster_label', \n",
    "           fit_reg=False, data=df_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's first revisit the 12-point toy dataset.\n",
    "plt.scatter(df_xy.x, df_xy.y, s=70)\n",
    "for i, xi in enumerate(df_xy.x):\n",
    "    plt.annotate(str(i), (xi+0.03, df_xy.y[i]+0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform k-means using the scikit-learn package\n",
    "kmeans_model = skc.KMeans(n_clusters=3)\n",
    "kmeans_model.fit(df_xy)\n",
    "centroids = kmeans_model.cluster_centers_\n",
    "centroids  # these are the centers of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See cluster_labels\n",
    "kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the clusters with their centroids.\n",
    "df_xy['cluster_labels_kmeans'] = kmeans_model.labels_\n",
    "f = sns.lmplot(x='x', y='y', data=df_xy, hue='cluster_labels_kmeans',\n",
    "               fit_reg=False, size=5, aspect=1.3)\n",
    "f.ax.scatter(centroids[:,0], centroids[:,1], marker='+', s=100,\n",
    "             linewidths=2, color='k');\n",
    "_ = plt.title('12-point dataset with K-means centroids (k=3)', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traits = [\n",
    "    ('Alice', ['Accessible', 'Confident', 'Creative', 'Sweet']),\n",
    "    ('Bob', ['Strong', 'Confident', 'Creative','Tidy']),\n",
    "    ('Charlie', ['Confident', 'Strong', 'Accessible', 'Sweet']),\n",
    "    ('Dacy', ['Accessible', 'Strong', 'Tidy', 'Confident']),\n",
    "    ('Emily', ['Reserved', 'Cute', 'Quiet', 'Impassive']),\n",
    "    ('Fred', ['Cute', 'Impassive', 'Gentle','Confident']),\n",
    "    ('George', ['Cute', 'Quiet', 'Reserved', 'Confident'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In order to make computation easier, let's create dummy variables \n",
    "# for traits. You should be able to understand what is going on below.\n",
    "\n",
    "# First, obtain a unique list of traits.\n",
    "all_traits = sorted({t for x in traits for t in x[1]})\n",
    "\n",
    "# Second, create dummy variables.\n",
    "traits_dummy = []\n",
    "for name, ts in traits:\n",
    "    tis = [0] * len(all_traits)\n",
    "    for t in ts:\n",
    "        tis[all_traits.index(t)] = 1\n",
    "    traits_dummy.append(tis)\n",
    "\n",
    "# Third, create a DataFrame.\n",
    "names = [x[0] for x in traits]\n",
    "df_traits = pd.DataFrame(traits_dummy, columns=all_traits, index=names)\n",
    "df_traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_model = skc.KMeans(n_clusters=3)\n",
    "kmeans_model.fit(df_traits)\n",
    "centroids = kmeans_model.cluster_centers_\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different distance measures between Alice and Bob.\n",
    "We've copied their traits down here:\n",
    "\n",
    " ('Alice', ['Accessible', 'Confident', 'Creative', 'Sweet']),\n",
    "\n",
    " ('Bob', ['Strong', 'Confident', 'Creative','Sporting']),\n",
    "\n",
    "Notice they have 4 traits each and they share 2 traits in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (1) Euclidean distance:\n",
    "#     by definition, this will be sqrt(1^2 + 1^2 + 1^2 + 1^2)\n",
    "spd.euclidean(df_traits.loc['Alice'], df_traits.loc['Bob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (2) Jaccard distance:\n",
    "#     by definition, this will be 1 - (2 / 6)\n",
    "spd.jaccard(df_traits.loc['Alice'], df_traits.loc['Bob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (3) Manhattan (cityblock) distance:\n",
    "#     by definition, this will be 1 + 1 + 1 + 1\n",
    "spd.cityblock(df_traits.loc['Alice'], df_traits.loc['Bob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (4) Cosine distance\n",
    "#     by definition, this will be 1 - 2 / (2 * 2)\n",
    "spd.cosine(df_traits.loc['Alice'], df_traits.loc['Bob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we have seen in Section 1, you can compute pairwise distance like\n",
    "# below, using any of the distance measurements. \n",
    "dist_traits = spd.squareform(spd.pdist(df_traits, 'cosine'))\n",
    "dist_traits[:4,:4]  # to save space, only show the top left 4x4 block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Determining the Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of clustering methods, such as k-means, assumes the parameter _k_ (#clusters) is known in advance, which is often not the case in practice. A number of techniques exist for determining the number of clusters in a dataset. See [this Wikipedia page](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Information_Criterion_Approach) for a detailed discussion.\n",
    "\n",
    "In this section, we focus on four of the approaches:\n",
    "1. Rule of thumb\n",
    "2. The Elbow Method\n",
    "\n",
    "For this section, let us use `df_xy`, the dataset we created in Section 1. Obviously, there are 3 natural clusters in the dataset. Let us see if all the methods listed above will be able to recover the true number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Rule of thumb:\n",
    "Choosing the number of clusters to simply be\n",
    "\n",
    "$$\n",
    "k \\approx \\sqrt{n/2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall df_xy used in the first section.\n",
    "plt.scatter(df_xy.x, df_xy.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To determine the natural cluster\n",
    "np.sqrt(len(df_xy) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Round this number to an integer\n",
    "np.round(np.sqrt(len(df_xy) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we desired. We wanted k to be 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Elbow Method\n",
    "See [here](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method) for an explanation.\n",
    "\n",
    "Implementation Reference: https://github.com/nborwankar/LearnDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_xy.iloc[:,:2]  # Using the the dataset of Section 1\n",
    "K = range(1,11)  # Apply kmeans 1 to 10\n",
    "kmeans_models = [skc.KMeans(k).fit(X) for k in K]\n",
    "centroids = [m.cluster_centers_ for m in kmeans_models]\n",
    "D_k = [spd.cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "dist = [np.min(D,axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "# plot elbow curve\n",
    "plt.plot(K, avgWithinSS, 'b*-')\n",
    "plt.xlabel('Number of clusters');\n",
    "plt.ylabel('Average within-cluster sum of squares');\n",
    "plt.title('Elbow for K-Means clustering');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graph above, the \"within-cluster sum of squares\" (i.e., within-cluster variance) reduces as the number of clusters (k) grows. However, as k grows larger, the marginal reduction drops. At some point, this drop is so obvious that it forms a visible \"angle\" in the graph. The number of clusters is then chosen at this point, hence the \"elbow\" criterion\". The \"elbow\" is not always easily identifiable.\n",
    "\n",
    "In the above plot, we can manually identify k=3 as the \"elbow\" point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Coursera Jeff Leek's \"Data Analysis\" Course Notes: https://github.com/herrfz/dataanalysis\n",
    "- Scikit-learn clustering tutorial: http://scikit-learn.org/stable/modules/clustering.html\n",
    "- Manning et al. [\"Evaluation of Clustering\"](http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html). _Introduction to Information Retrieval_ (2008)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
